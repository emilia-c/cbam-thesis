{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "\n",
    "# language detection\n",
    "from langdetect import detect\n",
    "\n",
    "# text pre-processing \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Stakeholder Scraped Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Stakeholder HTML Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files detected: 1487\n",
      "    file_id  org file_num                                            content  \\\n",
      "0  102_1084  102     1084  Civil society organisations express concerns a...   \n",
      "1  102_1088  102     1088  Societies and businesses face increasing uncer...   \n",
      "2  102_1091  102     1091  EU’s ‘Fit for 55’ is unfit and unfair The Euro...   \n",
      "3  102_1095  102     1095  WHEN: 12 December 2024 I TIME: 14:00 - 15:30 (...   \n",
      "4  102_1096  102     1096  Climate and Energy WG meeting – 14 September –...   \n",
      "\n",
      "          data_type  \n",
      "0  stakeholder_html  \n",
      "1  stakeholder_html  \n",
      "2  stakeholder_html  \n",
      "3  stakeholder_html  \n",
      "4  stakeholder_html  \n",
      "     file_id org file_num                                            content  \\\n",
      "1144   9_141   9      141  The past three years have been very crucial fo...   \n",
      "1145   9_142   9      142  Minespider announces strategic partnership wit...   \n",
      "1146   9_143   9      143  Executive Vice President and Chief Commercial ...   \n",
      "1147   9_145   9      145  Aluminum MMI: Buyers see tight supply, rising ...   \n",
      "1148   9_147   9      147  Hindalco to invest ₹45,000 crore in aluminium,...   \n",
      "\n",
      "             data_type  \n",
      "1144  stakeholder_html  \n",
      "1145  stakeholder_html  \n",
      "1146  stakeholder_html  \n",
      "1147  stakeholder_html  \n",
      "1148  stakeholder_html  \n"
     ]
    }
   ],
   "source": [
    "# first stakeholders html \n",
    "stakeholder_html_dir = \"master_thesis_2025/stakeholder_data_extraction_pipeline/data/processed_data/html_text\"\n",
    "\n",
    "stakeholder_html_list = []\n",
    "data_type = \"stakeholder_html\"\n",
    "#languages_detected = []\n",
    "\n",
    "for root, dirs, files in os.walk(stakeholder_html_dir):\n",
    "    # check that files are detected\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(stakeholder_html_dir))\n",
    "    print(f\"Total files detected: {file_count}\")\n",
    "\n",
    "    for f in files:\n",
    "        full_file_path = (os.path.join(root, f)) # save full file path\n",
    "        \n",
    "        # extract file_id, org_num, file_num using regex\n",
    "        match = re.match(r\"(\\d+)_(\\d+)\\.json\", f)\n",
    "        if match:\n",
    "            org_num, file_num = match.groups()\n",
    "            file_id = f\"{org_num}_{file_num}\"\n",
    "        else:\n",
    "            print(f\"Skipping {f}: Filename format not recognized\")\n",
    "            continue\n",
    "        \n",
    "        with open(full_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                if \"text\" in data:\n",
    "                    # extract text from json\n",
    "                    text_content = data[\"text\"]\n",
    "                    \n",
    "                    # detect language\n",
    "                    lang = detect(text_content) \n",
    "                    #languages_detected.append(lang)\n",
    "                    #print(lang)\n",
    "                    \n",
    "                    # FOR NOW ONLY EXTRACT ENGLISH TEXTS\n",
    "                    if lang == 'en':\n",
    "                        stakeholder_html_list.append({\n",
    "                            \"file_id\": file_id,\n",
    "                            \"org\": org_num,\n",
    "                            \"file_num\": file_num,\n",
    "                            \"content\": text_content, \n",
    "                            \"data_type\": data_type\n",
    "                        })\n",
    "                else: \n",
    "                    print(f'{full_file_path} no text')\n",
    "            \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"error {full_file_path}: {e}\")\n",
    "\n",
    "# convert the list to df\n",
    "html_stakeholder = pd.DataFrame(stakeholder_html_list)\n",
    "print(html_stakeholder.head())\n",
    "print(html_stakeholder.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 PDF Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files detected: 481\n",
      "Skipping 222_2149.json due to keywords\n",
      "    file_id  org file_num                                            content  \\\n",
      "0  100_1060  100     1060  if\\n8:\\nS\\n| ‘|\\n! ry\\nZ,\\nY\\nUp\\ni\\nA\\nPo\\n\\\\...   \n",
      "1  101_1066  101     1066  Carbon Border Adjustments: Climate Protection ...   \n",
      "2  101_1068  101     1068  Carbon\\nMarket\\nWatch\\nA brief explanation of ...   \n",
      "3  101_1072  101     1072  Carbon\\nMarket\\nWatch\\nA brief explanation of ...   \n",
      "4  101_1073  101     1073  O Carbon\\nMarket\\nWatch\\nCarbon Market Watch’s...   \n",
      "\n",
      "         data_type  \n",
      "0  stakeholder_pdf  \n",
      "1  stakeholder_pdf  \n",
      "2  stakeholder_pdf  \n",
      "3  stakeholder_pdf  \n",
      "4  stakeholder_pdf  \n",
      "     file_id org file_num                                            content  \\\n",
      "385   91_998  91      998  ©) MATERIALS\\nBOLIDEN\\nBoliden — Metals for\\ng...   \n",
      "386  92_1008  92     1008  A Close Brothers\\nNOT FOR RELEASE, PUBLICATION...   \n",
      "387  92_1015  92     1015  A Close Brothers\\nNOT FOR RELEASE, PUBLICATION...   \n",
      "388  92_1020  92     1020  a KAPSARC\\nKing Abel JS) SgasJlg oe slate oe K...   \n",
      "389  93_1029  93     1029  @®ECGA\\nEuropean Carbon and Graphite Associati...   \n",
      "\n",
      "           data_type  \n",
      "385  stakeholder_pdf  \n",
      "386  stakeholder_pdf  \n",
      "387  stakeholder_pdf  \n",
      "388  stakeholder_pdf  \n",
      "389  stakeholder_pdf  \n"
     ]
    }
   ],
   "source": [
    "# then read in stakeholders pdf\n",
    "stakeholder_pdf_dir = \"master_thesis_2025/stakeholder_data_extraction_pipeline/data/processed_data/pdf_text\"\n",
    "\n",
    "# keywords to skip (files containg these will be ignored)\n",
    "skip_keywords = ['Concerns-Based Adoption Model']  # found overlap manually, also called CBAM\n",
    "\n",
    "# list to store processed data\n",
    "stakeholder_pdf_list = []\n",
    "data_type = 'stakeholder_pdf'\n",
    "\n",
    "# walk through json directory\n",
    "for root, dirs, files in os.walk(stakeholder_pdf_dir):\n",
    "    # check that files are detected\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(stakeholder_pdf_dir))\n",
    "    print(f\"Total files detected: {file_count}\")\n",
    "\n",
    "    for f in files:\n",
    "            full_file_path = os.path.join(root, f)  # save full file path\n",
    "            # extract file_id, org_num, file_num using regex\n",
    "            match = re.match(r\"(\\d+)_(\\d+)\\.json\", f)\n",
    "            if match:\n",
    "                org_num, file_num = match.groups()\n",
    "                file_id = f\"{org_num}_{file_num}\"\n",
    "            else:\n",
    "                print(f\"Skipping {f}: Filename format not recognized\")\n",
    "                continue\n",
    "            \n",
    "            with open(full_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                try:\n",
    "                    data = json.load(file)\n",
    "                    \n",
    "                    # ensure the necessary keys exist and process only English-language documents\n",
    "                    if 'language' in data and 'pages' in data and data['language'] == 'Languages.ENGLISH':\n",
    "                        full_text = \" \".join(data['pages'].values())\n",
    "                        \n",
    "                        # Skip if any of the skip keywords are found in the text\n",
    "                        if any(keyword.lower() in full_text.lower() for keyword in skip_keywords):\n",
    "                            print(f\"Skipping {f} due to keywords\")\n",
    "                            continue\n",
    "                        \n",
    "                        stakeholder_pdf_list.append({\n",
    "                            \"file_id\": file_id,\n",
    "                            \"org\": org_num,\n",
    "                            \"file_num\": file_num,\n",
    "                            \"content\": full_text, \n",
    "                            \"data_type\": data_type\n",
    "                        })\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"error {f}: {e}\")\n",
    "\n",
    "# convert the list to df\n",
    "pdf_stakeholder = pd.DataFrame(stakeholder_pdf_list)\n",
    "print(pdf_stakeholder.head())\n",
    "print(pdf_stakeholder.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 EC Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         file_id  \\\n",
      "0        CELEX_32023R0956_EN_TXT   \n",
      "1        CELEX_32023R1773_EN_TXT   \n",
      "2  COM_2025_87_1_EN_ACT_part1_v5   \n",
      "3         COM_2025_87_annexes_EN   \n",
      "4          OJ_L_202403210_EN_TXT   \n",
      "\n",
      "                                             content       data_type  \n",
      "0  L 130/52\\nEN\\n16.5.2023\\nOfficial Journal of t...  ec_legislation  \n",
      "1  L 228/94\\nEN\\nOfficial Journal of the European...  ec_legislation  \n",
      "2  a\\nEUROPEAN\\nCOMMISSION\\nBrussels, 26.2.2025\\n...  ec_legislation  \n",
      "3  pi\\nEUROPEAN\\nCOMMISSION\\nBrussels, 26.2.2025 ...  ec_legislation  \n",
      "4  EA Official Journal\\nof the European Union\\n20...  ec_legislation  \n",
      "                         file_id  \\\n",
      "1        CELEX_32023R1773_EN_TXT   \n",
      "2  COM_2025_87_1_EN_ACT_part1_v5   \n",
      "3         COM_2025_87_annexes_EN   \n",
      "4          OJ_L_202403210_EN_TXT   \n",
      "5              SWD-Omnibus-87_En   \n",
      "\n",
      "                                             content       data_type  \n",
      "1  L 228/94\\nEN\\nOfficial Journal of the European...  ec_legislation  \n",
      "2  a\\nEUROPEAN\\nCOMMISSION\\nBrussels, 26.2.2025\\n...  ec_legislation  \n",
      "3  pi\\nEUROPEAN\\nCOMMISSION\\nBrussels, 26.2.2025 ...  ec_legislation  \n",
      "4  EA Official Journal\\nof the European Union\\n20...  ec_legislation  \n",
      "5  Ee\\nEUROPEAN\\nCOMMISSION\\nBrussels, 26.2.2025\\...  ec_legislation  \n"
     ]
    }
   ],
   "source": [
    "# first EC legislation\n",
    "ec_leg_file_path = r\"master_thesis_2025\\eu_data_extraction\\EC\\legislation\\data\\results\\legislation_data.json\"\n",
    "\n",
    "# List to store extracted data\n",
    "ec_leg_data_list = []\n",
    "data_type = \"ec_legislation\"\n",
    "#org = \"org\"\n",
    "\n",
    "# Read JSON file\n",
    "with open(ec_leg_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"text\" in entry:\n",
    "                text_content = entry[\"text\"]\n",
    "\n",
    "                # Detect language\n",
    "                lang = detect(text_content)\n",
    "\n",
    "                # Only include English texts\n",
    "                if lang == \"en\":\n",
    "                    ec_leg_data_list.append({\n",
    "                        \"file_id\": entry.get(\"title\", \"Unknown\"),  # Use \"title\" as file_id\n",
    "                        \"content\": text_content,\n",
    "                        \"data_type\": data_type\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"{ec_leg_file_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ec_leg_file_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "legislation_df = pd.DataFrame(ec_leg_data_list)\n",
    "\n",
    "print(legislation_df.head())\n",
    "print(legislation_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             file_id  \\\n",
      "0  Remarks by Executive Vice-President Dombrovski...   \n",
      "1  Joint statement on the second meeting of the E...   \n",
      "2  Commission simplifies rules on sustainability ...   \n",
      "3  A Clean Industrial Deal for competitiveness an...   \n",
      "4  Speech at European Economic and Social Committ...   \n",
      "\n",
      "                                             content         data_type  \n",
      "0  MerciBruno, dear colleagues,\\nAs Russia's aggr...  ec_press_release  \n",
      "1  The second meeting of the EU-India Trade and T...  ec_press_release  \n",
      "2  The European Commission has adopted a new pack...  ec_press_release  \n",
      "3  Today, the Commission presents theClean Indust...  ec_press_release  \n",
      "4  Madame President, Honourable Members,\\nI am re...  ec_press_release  \n",
      "                                              file_id  \\\n",
      "56  Opening address by President von der Leyen on ...   \n",
      "57  Frans Timmermans at Parliament Plenary session...   \n",
      "58  Closing Remarks EVP Timmermans on Fit for 55 a...   \n",
      "59  Pour une Europe plus compétitive et autonome, ...   \n",
      "60                          Daily News 14 / 11 / 2023   \n",
      "\n",
      "                                              content         data_type  \n",
      "56  Thank you, dear Dr Kadri, for the invitation.\\...  ec_press_release  \n",
      "57  Openingremarks\\nThank you, Mr. President, hono...  ec_press_release  \n",
      "58  Si vous permettez madame la présidente, pas po...  ec_press_release  \n",
      "59  Mesdames et Messieurs les députés,\\nJe suis he...  ec_press_release  \n",
      "60  EU is providing additional €110 million in hum...  ec_press_release  \n"
     ]
    }
   ],
   "source": [
    "# then EC press releases\n",
    "ec_press_release_path = r\"master_thesis_2025\\eu_data_extraction\\EC\\press_release\\data\\EC_CBAM_articles.json\"\n",
    "\n",
    "# list to store extracted data\n",
    "ec_press_release_list = []\n",
    "data_type = \"ec_press_release\"\n",
    "\n",
    "# read JSON file\n",
    "with open(ec_press_release_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"text\" in entry:\n",
    "                text_content = entry[\"text\"]\n",
    "\n",
    "                # detect language\n",
    "                #lang = detect(text_content)\n",
    "\n",
    "                # only include English texts (only english news included)\n",
    "                #if lang == \"en\":\n",
    "                ec_press_release_list.append({\n",
    "                    \"file_id\": entry.get(\"title\", \"Unknown\"),  # Use \"title\" as file_id\n",
    "                    \"content\": text_content,\n",
    "                    \"data_type\": data_type\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{ec_press_release_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ec_press_release_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "ec_pressrelease_df = pd.DataFrame(ec_press_release_list)\n",
    "\n",
    "print(ec_pressrelease_df.head())\n",
    "print(ec_pressrelease_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  file_id                                            content data_type\n",
      "0  525305                                 See attached file.    ec_hys\n",
      "1  525248                                               n.a.    ec_hys\n",
      "2  525305                                 See attached file.    ec_hys\n",
      "3  525248                                               n.a.    ec_hys\n",
      "4  525246   AEGIS Europe is an industry alliance that bri...    ec_hys\n",
      "     file_id                                            content data_type\n",
      "398  3497854  Hello Dear Madam/Sir We are reaching you about...    ec_hys\n",
      "399  3497758  The delivery of the information from the suppl...    ec_hys\n",
      "400  3497706  The correct functioning of the CBAM Regulation...    ec_hys\n",
      "401  3497693  Around chapter III, I believe the objective fo...    ec_hys\n",
      "402  3497684  the register should promote sensible climate a...    ec_hys\n"
     ]
    }
   ],
   "source": [
    "# then EC have your say (feedback)\n",
    "ec_hys_path = r\"master_thesis_2025\\eu_data_extraction\\EC\\have_your_say\\data\\combined_hys.json\"\n",
    "\n",
    "# list to store extracted data\n",
    "ec_hys_list = []\n",
    "data_type = \"ec_hys\"\n",
    "\n",
    "# read JSON file\n",
    "with open(ec_hys_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"feedback\" in entry:\n",
    "                # combine 'feedback' and 'pdf_text' if they exist\n",
    "                combined_text = \" \".join(filter(None, [entry.get(\"feedback\", \"\"), entry.get(\"pdf_text\", \"\")]))\n",
    "\n",
    "            #    text_content = entry[\"text\"]\n",
    "\n",
    "                # detect language\n",
    "                #lang = detect(text_content)\n",
    "\n",
    "                # only include English texts\n",
    "                #if lang == \"en\":\n",
    "                ec_hys_list.append({\n",
    "                    \"file_id\": entry.get(\"file_id\"),  \n",
    "                    \"content\": combined_text,\n",
    "                    \"data_type\": data_type\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{ec_hys_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ec_hys_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "ec_hys_df = pd.DataFrame(ec_hys_list)\n",
    "\n",
    "print(ec_hys_df.head())\n",
    "print(ec_hys_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 EP Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            file_id  \\\n",
      "0                         1244098EN   \n",
      "1                COM_COM20210564_EN   \n",
      "2                         1270822EN   \n",
      "3                COM_COM20220101_EN   \n",
      "4  FINALVotingListMAR3FdR1270822_EN   \n",
      "\n",
      "                                             content              data_type  \n",
      "0  European Parliament\\n2019-2024\\n((( = =—r\\n(4\\...  ep_committee_meetings  \n",
      "1  pa\\nEUROPEAN\\nCOMMISSION\\nBrussels, 14.7.2021\\...  ep_committee_meetings  \n",
      "2  European Parliament\\n2019-2024\\n((( = =—r\\n(4\\...  ep_committee_meetings  \n",
      "3  a\\nEUROPEAN\\nCOMMISSION\\nBrussels, 14.3.2022\\n...  ep_committee_meetings  \n",
      "4  European Parliament\\n2019-2024\\n7 SN J ANA\\nSN...  ep_committee_meetings  \n",
      "               file_id                                            content  \\\n",
      "78           1248940EN  European Parliament\\n2019-2024\\n(( =—r\\nSS w=\\...   \n",
      "79           1248941EN  European Parliament\\n2019-2024\\n(( =—r\\nSS w=\\...   \n",
      "80         CAs_CBAM_EN  European Parliament\\n2019 - 2024\\nG G SS GES\\n...   \n",
      "81  COM_COM20210564_EN  pa\\nEUROPEAN\\nCOMMISSION\\nBrussels, 14.7.2021\\...   \n",
      "82           VLCBAM_EN  <= SS\\nCo SANA\\nS ‘\\n— =\\n-\\n=\\n—\\nEuropean Pa...   \n",
      "\n",
      "                data_type  \n",
      "78  ep_committee_meetings  \n",
      "79  ep_committee_meetings  \n",
      "80  ep_committee_meetings  \n",
      "81  ep_committee_meetings  \n",
      "82  ep_committee_meetings  \n"
     ]
    }
   ],
   "source": [
    "# first the committee meetings \n",
    "ep_comm_path = r\"master_thesis_2025\\eu_data_extraction\\EP\\committee_meetings\\committee_meetings_data.json\"\n",
    "# list to store extracted data\n",
    "ep_comm_list = []\n",
    "data_type = \"ep_committee_meetings\"\n",
    "\n",
    "# read JSON file\n",
    "with open(ep_comm_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"text\" in entry:\n",
    "                text_content = entry[\"text\"]\n",
    "\n",
    "                ep_comm_list.append({\n",
    "                    \"file_id\": entry.get(\"title\"),  # use file title as file id\n",
    "                    \"content\": text_content,\n",
    "                    \"data_type\": data_type\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{ep_comm_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ep_comm_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "ep_comm_df = pd.DataFrame(ep_comm_list)\n",
    "\n",
    "print(ep_comm_df.head())\n",
    "print(ep_comm_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             file_id  \\\n",
      "0  European Parliament Press Kit for the European...   \n",
      "1  \"Own Resources\": Parliament's position on new ...   \n",
      "2  MEPs clear way for new EU revenue, call on EU ...   \n",
      "3  MEPs urge member states to adopt EU income str...   \n",
      "4  Fit for 55: Parliament adopts key laws to reac...   \n",
      "\n",
      "                                             content          data_type  \n",
      "0  In this press kit, you will find a selection o...  ep_press_releases  \n",
      "1  Parliament has always been an advocate of new ...  ep_press_releases  \n",
      "2  New EU revenue based on the Emissions Trading ...  ep_press_releases  \n",
      "3  Following the presentation of updated proposal...  ep_press_releases  \n",
      "4  Free allowances in the Emissions Trading Syste...  ep_press_releases  \n",
      "                                              file_id  \\\n",
      "10  MEPs to G20: increase climate change targets b...   \n",
      "11  CBAM: Parliament pushes for higher ambition in...   \n",
      "12  Fit for 55 in 2030: Parliament wants a more am...   \n",
      "13         COP26 deal: Reactions from MEPs in Glasgow   \n",
      "14  MEPs: Put a carbon price on certain EU imports...   \n",
      "\n",
      "                                              content          data_type  \n",
      "10  EU should show leadership by raising its 2030 ...  ep_press_releases  \n",
      "11  Phasing in CBAM from 2027 and ending free allo...  ep_press_releases  \n",
      "12  New ETS II for commercial buildings and road t...  ep_press_releases  \n",
      "13  Following the deal reached in Glasgow, the MEP...  ep_press_releases  \n",
      "14  All products under the EU Emissions Trading Sy...  ep_press_releases  \n"
     ]
    }
   ],
   "source": [
    "# then ep press releases\n",
    "ep_pr_path = r\"master_thesis_2025\\eu_data_extraction\\EP\\press_release\\data\\ep_pressroom\\EP_CBAM_articles.json\"\n",
    "\n",
    "# list to store extracted data\n",
    "ep_pr_list = []\n",
    "data_type = \"ep_press_releases\"\n",
    "\n",
    "# read JSON file\n",
    "with open(ep_pr_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"text\" in entry:\n",
    "                text_content = entry[\"text\"]\n",
    "\n",
    "                ep_pr_list.append({\n",
    "                    \"file_id\": entry.get(\"title\"),  # use file title as file id\n",
    "                    \"content\": text_content,\n",
    "                    \"data_type\": data_type\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{ep_pr_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ep_pr_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "ep_pr_df = pd.DataFrame(ep_pr_list)\n",
    "\n",
    "print(ep_pr_df.head())\n",
    "print(ep_pr_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             file_id  \\\n",
      "0  Prevent unfair competition by non-European cli...   \n",
      "1          De-carbonise, not de-industrialise Europe   \n",
      "2  Fit for 55: we want to de-carbonise, not de-in...   \n",
      "3         We are increasing CO2 cuts from 55% to 57%   \n",
      "4                  EU Budget is running out of money   \n",
      "\n",
      "                                             content                data_type  \n",
      "0  The EPP Group wants to introduce a Carbon Bord...  ep_group_press_releases  \n",
      "1  The EPP Group fully supports the move towards ...  ep_group_press_releases  \n",
      "2  On Tuesday, the European Parliament’s plenary ...  ep_group_press_releases  \n",
      "3  The EPP Group has greatly improved the Fit for...  ep_group_press_releases  \n",
      "4  In just four years’ time, vital EU funds suppo...  ep_group_press_releases  \n",
      "                                              file_id  \\\n",
      "39  European Parliament approves catalyst for clea...   \n",
      "40  Commission proposals welcome but not enough to...   \n",
      "41  MEPs prepare to slate summit deal & defend rea...   \n",
      "42  It is five past twelve: vote on the Fit for 55...   \n",
      "43               The deregulation for the rich agenda   \n",
      "\n",
      "                                              content                data_type  \n",
      "39  Carbon Border Adjustment Mechanism (CBAM) Toda...  ep_group_press_releases  \n",
      "40  MFF/Own Resources Today, the European Commissi...  ep_group_press_releases  \n",
      "41  Decrying EU leaders’ hatchet job on the long-t...  ep_group_press_releases  \n",
      "42  In 2015, at the climate summit in Paris, the E...  ep_group_press_releases  \n",
      "43  The Clean Industrial Deal and Omnibus package ...  ep_group_press_releases  \n"
     ]
    }
   ],
   "source": [
    "# then politic group press releases\n",
    "ep_group_pr_path = r\"master_thesis_2025\\eu_data_extraction\\EP\\press_release\\data\\political_groups\\group_press_releases.json\"\n",
    "# list to store extracted data\n",
    "ep_group_pr_list = []\n",
    "data_type = \"ep_group_press_releases\"\n",
    "\n",
    "# read JSON file\n",
    "with open(ep_group_pr_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    try:\n",
    "        data = json.load(file)  # Load JSON file\n",
    "        for entry in data:\n",
    "            if \"text\" in entry:\n",
    "                text_content = entry[\"text\"]\n",
    "\n",
    "                ep_group_pr_list.append({\n",
    "                    \"file_id\": entry.get(\"title\"),  # use file title as file id\n",
    "                    \"content\": text_content,\n",
    "                    \"data_type\": data_type\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{ep_group_pr_path} has no text\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading {ep_group_pr_path}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "ep_group_pr_df = pd.DataFrame(ep_group_pr_list)\n",
    "\n",
    "print(ep_group_pr_df.head())\n",
    "print(ep_group_pr_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file_id  org file_num                                            content  \\\n",
      "0  102_1084  102     1084  Civil society organisations express concerns a...   \n",
      "1  102_1088  102     1088  Societies and businesses face increasing uncer...   \n",
      "2  102_1091  102     1091  EU’s ‘Fit for 55’ is unfit and unfair The Euro...   \n",
      "3  102_1095  102     1095  WHEN: 12 December 2024 I TIME: 14:00 - 15:30 (...   \n",
      "4  102_1096  102     1096  Climate and Energy WG meeting – 14 September –...   \n",
      "\n",
      "          data_type  \n",
      "0  stakeholder_html  \n",
      "1  stakeholder_html  \n",
      "2  stakeholder_html  \n",
      "3  stakeholder_html  \n",
      "4  stakeholder_html  \n",
      "number of documents: 2151\n"
     ]
    }
   ],
   "source": [
    "all_docs = pd.concat([html_stakeholder,     # scraped stakeholders\n",
    "                      pdf_stakeholder,      # scraped stakeholders \n",
    "                      legislation_df,       # legislation \n",
    "                      ec_hys_df,            # ec feedback\n",
    "                      ec_pressrelease_df,   # ec press releases\n",
    "                      ep_comm_df,           # ep committee docs\n",
    "                      ep_pr_df,             # ep press releases\n",
    "                      ep_group_pr_df        # political group press releases   \n",
    "                      ], ignore_index=True)\n",
    "print(all_docs.head())\n",
    "#print(all_docs.tail())\n",
    "\n",
    "# Convert 'content' column to a list of Unicode strings\n",
    "docs = all_docs[\"content\"].astype(str).tolist()\n",
    "print(f'number of documents: {len(docs)}')\n",
    "#print(docs[0][:500])\n",
    "#print(docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU's 'Fit for 55' is unfit and unfair and must modernize. The European Commission is missing another historic opportunity to phase out fossil fuels in the 'Fit for 55' package, leaving the door open for coal, gas and oil to stay in the EU energy system for at least another two decades while sending the \"polluter pays\" bill to EU citizens.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from cleantext import clean\n",
    "import wordninja  # for word segmentation\n",
    "\n",
    "# Function to clean OCR noise\n",
    "def clean_ocr_noise(text):\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)          # Remove hyphenated line breaks\n",
    "    text = re.sub(r'\\n+', ' ', text)               # Replace newlines with space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)            # Reduce multiple spaces\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)      # Remove non-ASCII chars (optional)\n",
    "    return text\n",
    "\n",
    "# Function to clean repeated characters\n",
    "def clean_repeated_chars(text):\n",
    "    text = re.sub(r'\\b(\\w)\\1{2,}\\b', '', text)  # Remove entire words with repeated chars (e.g., \"aaa\")\n",
    "    #text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text) # Limit repeated characters within words to 2 max (e.g., \"soooon\" -> \"soon\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Function to remove junk tokens (e.g., short tokens, numbers, non-alphabetic tokens)\n",
    "def remove_junk_tokens(doc):\n",
    "    return [\n",
    "        token for token in doc\n",
    "        if len(token) > 2                          # remove very short tokens\n",
    "        and not re.match(r'^[a-z]{1,2}$', token)   # remove tokens like \"ae\", \"yy\", etc.\n",
    "        and not re.search(r'\\d', token)            # remove tokens with numbers\n",
    "        and token.isalpha()                        # keep only alphabetic tokens\n",
    "    ]\n",
    "\n",
    "# Function to remove hex strings and IDs (e.g., long hex strings, long numeric IDs)\n",
    "def remove_hex_and_ids(doc):\n",
    "    return [\n",
    "        token for token in doc\n",
    "        if not re.fullmatch(r'[a-f0-9]{8,}', token)   # long hex/hash strings\n",
    "        and not re.fullmatch(r'\\d{5,}', token)        # long numeric IDs\n",
    "        #and not re.fullmatch(r'[a-z]{1,2}_[a-z]{1,2}(_[a-z]{1,2})*', token)  # ee_ee_ee etc\n",
    "    ]\n",
    "\n",
    "# Function for keyword-specific British-to-American spelling remapping\n",
    "def british_to_american_keywords(text):\n",
    "    british_to_american_dict = {\n",
    "        \"organisations\": \"organizations\",\n",
    "        \"organisation\": \"organization\",\n",
    "        \"realisation\": \"realization\",\n",
    "        \"digitalisation\": \"digitalization\",\n",
    "        \"decarbonisation\": \"decarbonization\",\n",
    "        \"recognise\": \"recognize\",\n",
    "        \"analyse\": \"analyze\",\n",
    "        \"labour\": \"labor\",\n",
    "        \"centre\": \"center\",\n",
    "        \"theatre\": \"theater\",\n",
    "        \"favour\": \"favor\",\n",
    "        \"colour\": \"color\",\n",
    "        \"honour\": \"honor\",\n",
    "        \"metre\": \"meter\",\n",
    "        \"defence\": \"defense\",\n",
    "        \"licence\": \"license\",\n",
    "        \"programme\": \"program\",\n",
    "        \"travelling\": \"traveling\",\n",
    "        \"realise\": \"realize\",\n",
    "        \"defence\": \"defense\",\n",
    "        \"theatre\": \"theater\", \n",
    "        \"modernise\": \"modernize\"\n",
    "    }\n",
    "    \n",
    "    for british, american in british_to_american_dict.items():\n",
    "        text = re.sub(r'\\b' + british + r'\\b', american, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to clean the text (apply all the steps)\n",
    "def clean_text(text):\n",
    "    # Use clean() from cleantext to remove URLs, emails, phone numbers, etc.\n",
    "    text = clean(\n",
    "        text=text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False, # has to be removed, too strong\n",
    "        no_emails=True,\n",
    "        no_urls=True,\n",
    "        no_line_breaks=True,\n",
    "        no_phone_numbers=True,\n",
    "        lower=False,            \n",
    "        #replace_with_punct=\"\", \n",
    "        replace_with_url=\"\", \n",
    "        replace_with_email=\"\"\n",
    "    )\n",
    "    \n",
    "    # Remove OCR noise and fix repeated characters\n",
    "    text = clean_ocr_noise(text)\n",
    "    text = clean_repeated_chars(text)\n",
    "\n",
    "    # Handle missing whitespace in concatenated words (e.g., \"adesire\" -> \"a desire\")\n",
    "    #text = ' '.join(wordninja.split(text))  # This will split concatenated words\n",
    "\n",
    "    # Apply keyword-specific British-to-American spelling remapping\n",
    "    text = british_to_american_keywords(text)\n",
    "    \n",
    "    # Tokenize the text and apply junk removal functions\n",
    "    tokens = text.split()  # Simple split by whitespace to get tokens\n",
    "    \n",
    "    # Remove junk tokens (short tokens, numbers, etc.)\n",
    "    #tokens = remove_junk_tokens(tokens)\n",
    "    \n",
    "    # Remove hex and ID-like tokens\n",
    "    tokens = remove_hex_and_ids(tokens)\n",
    "    \n",
    "    # Rejoin tokens back into a cleaned string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "#sample_text = \"\"\"EU's Civil society organisations express concerns about the agreement reached in COREPER II on a Carbon Border Adjustment Mechanism (CBAM) compromise text that does not address important issues linked to the EU Emission Trading System (ETS) Directive and other key aspects of the Fit for 55 package. Together with other NGOs, we support a fair and effective CBAM, designed and implemented as an alternative to current EU ETS carbon leakage measures such as free allowances and indirect cost compensation.\"\"\"\n",
    "sample_text = \"\"\"\n",
    "EU’s ‘Fit for 55’ is unfit and unfair and must modernise. The European Commission is missing another historic opportunity to phase out fossil fuels in the ‘Fit for 55’ package, \n",
    "leaving the door open for coal, gas and oil to stay in the EU energy system for at least another two decades while sending the “polluter pays” bill to EU citizens.\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file_id                                            content  \\\n",
      "0  102_1084  Civil society organisations express concerns a...   \n",
      "1  102_1088  Societies and businesses face increasing uncer...   \n",
      "2  102_1091  EU’s ‘Fit for 55’ is unfit and unfair The Euro...   \n",
      "3  102_1095  WHEN: 12 December 2024 I TIME: 14:00 - 15:30 (...   \n",
      "4  102_1096  Climate and Energy WG meeting – 14 September –...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  Civil society organizations express concerns a...  \n",
      "1  Societies and businesses face increasing uncer...  \n",
      "2  EU's 'Fit for 55' is unfit and unfair The Euro...  \n",
      "3  WHEN: 12 December 2024 I TIME: 14:00 - 15:30 (...  \n",
      "4  Climate and Energy WG meeting 14 September Onl...  \n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function to the \"content\" column and store in a new column:\n",
    "# Assuming `all_docs` is your dataframe:\n",
    "all_docs['cleaned_content'] = all_docs['content'].apply(clean_text)\n",
    "\n",
    "# Optionally, inspect the cleaned content:\n",
    "print(all_docs[['file_id', 'content', 'cleaned_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel \n",
    "import openpyxl\n",
    "all_docs.to_excel(\"master_thesis_2025/RAW_all_comms.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 1539\n"
     ]
    }
   ],
   "source": [
    "docs = all_docs[\"cleaned_content\"].astype(str).tolist()\n",
    "print(f'number of documents: {len(docs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
